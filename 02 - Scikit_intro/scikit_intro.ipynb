{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A real Machine Learning Project\n",
    "In this lecture, you will go through an example project from start to end,\n",
    "as a real data scientist.\n",
    "\n",
    "Here is an overview of all the steps you are going to deal with.\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Discover and visualize the data to gain insights\n",
    "4. Prepare the data for Machine Learning Algorithms\n",
    "5. Select a model and train it\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system.\n",
    "\n",
    "## Look at the Big Picture and Frame the Problem\n",
    "The dataset we are going to work with is a very famous dataset, i.e., the _California Housing Prices_ dataset.\n",
    "\n",
    "It contains information about houses, located in a certain California district. \n",
    "It is  based on data collected from a 1990 California census.\n",
    "\n",
    "Please note that the dataset needs to be _cleaned_. \n",
    "There are some pre-processing steps required in order to make it suitable to address a Data Mining Task.\n",
    "\n",
    "*Attributes* \n",
    "\n",
    "The attributes of the dataset are rather self explanatory.\n",
    "\n",
    "* longitude\n",
    "* latitude\n",
    "* housing_median_age\n",
    "* total_rooms\n",
    "* total_bedrooms\n",
    "* population\n",
    "* households\n",
    "* median_income\n",
    "* median_house_value\n",
    "* ocean_proximity\n",
    "\n",
    "\n",
    "In the earliest stages of your project you need to understand what kind of problem you are\n",
    "approaching to.\n",
    "\n",
    "In this context, you are required to build a model capable of approximating the price of houses\n",
    "in a california district.\n",
    "\n",
    "More specifically, you wish to be able to predict the price associated with any given\n",
    "_block_. \n",
    "\n",
    "A block is a general small geographical unit.\n",
    "\n",
    "The model you are going to train will be able to predict the median housing price of any _block_  \n",
    "belonging  to the district this data refers to.\n",
    "\n",
    "Once the overall goal is established, you can address the details of the problem at hand.\n",
    "\n",
    "---\n",
    "\n",
    "In a real scenario, you need to understand how your model is meant to be used.\n",
    "\n",
    "For instance, your model may be either part of a wider pipeline or a standalone project.\n",
    "\n",
    "\n",
    "For instance, let's assume the model is eventually embedded in the following pipeline.\n",
    "\n",
    "![](img/pipeline.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Question**\n",
    "\n",
    "Think about the nature of the problem you are going to solve. \n",
    "What is the best way to model this problem?\n",
    "\n",
    "*Answer*: ....\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Once you decide how to tackle the problem, you need to find a suitable way of measuring performance of your model.\n",
    "\n",
    "In this context, the most commonly used metrics are: options are:\n",
    "\n",
    "> **Root Means Squared Error**\n",
    "    $$\n",
    "        RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2}\n",
    "    $$\n",
    "    \n",
    "> **Mean Absolute Error**\n",
    "    $$\n",
    "        MAE(X,h) = \\frac{1}{m} \\sum_{i=1}^{m} | h(x^{(i)}) - y^{(i)}|\n",
    "    $$\n",
    "    \n",
    "Both definitions denote a way of measuring distance between vectors.\n",
    "More specifically, RMSE corresponds to the *Euclidian distance* ($l_2$ norm)\n",
    "while the MAE corresponds to the *Manhattan norm* ($l_1$ distance).\n",
    "\n",
    "In general, higher norms are more sensitive to outliers.\n",
    "In fact, RMSE is more sensitive to outliers if compared with MAE.\n",
    "\n",
    "More generally the $l_k$ norm of a vector $v$ is defined as\n",
    "$$\n",
    "|| v_k|| = (\\sum_i v_i^k)^{\\frac{1}{k}}\n",
    "$$\n",
    "\n",
    "When $k$ increases it focuses more on large values.\n",
    "\n",
    "This is one of the main reason why we \n",
    "use techniques as feature _scaling_ and _standardization_ to\n",
    "enhance the performance of a training algorithm.\n",
    "\n",
    "For example, RMSE  works better when \n",
    "the data follow a bell-shaped curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"img\")\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def maybe_fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    # check wether the dataset is already\n",
    "    # in the filesytesm\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "\n",
    "    # make the request to download the compressed file    \n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    \n",
    "    # open and decompress the file\n",
    "    housing_tgz = tarfile.open(tgz_path) \n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a Quick Look at the Data Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "1.  ``total_bedrooms`` contains some missing values.\n",
    "\n",
    "2. All the attributes but ``ocean_proximity`` are numerical (as suggested by the ``object`` dtype).\n",
    "\n",
    "\n",
    "In fact, this feature contains text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``describe`` function provides useful information about the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, plots are more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some graphs useful for visualizing the distribution of\n",
    "# the values wrt to each individual feature\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "#save_fig(\"attribute_histogram_plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can draw some observations\n",
    "\n",
    "1. ``median_income`` - draw your considerations \n",
    "2. ``median_house_age`` and ``median_house_value``  - draw your consideration\n",
    "\n",
    "3. Look at the scales - draw your considerations \n",
    "4. Look at the shape of these distributions. Remember, RMSE works better with a bell-shaped curve.\n",
    "\n",
    "## Create a  Test Set\n",
    "\n",
    "It might seem weird to put aside part of the data already at this early stage.\n",
    "\n",
    "However, it is always a good practice to put aside some of  the data point, so  to prevent a phenomena known as _data snooping bias_. \n",
    "\n",
    "The idea of letting the data drive the entire process is tempting, however this approach may be counter-productive.\n",
    "\n",
    "For this reason,  you should always have a portion of the data that your algorithm has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# your code here\n",
    "# For illustration only. Sklearn has train_test_split()\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing the function yourself, you can rely  ``sklearn``.\n",
    "\n",
    "It comes with  a useful function for splitting your data into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data is a delicate and crucial task. \n",
    "\n",
    "The main thing you want to avoid is to introduce: _sampling bias_.\n",
    "Also, you want the data contained in the test set to be a \n",
    "__meaningful__ view on the original dataset.\n",
    "\n",
    "\n",
    "**Example**\n",
    "When a survey\n",
    "company decides to call 1,000 people to ask them a few questions, they don’t just pick\n",
    "1,000 people randomly in a phone booth. \n",
    "\n",
    "They try to ensure that these 1,000 people are representative of the whole population. \n",
    "\n",
    "For example, the US population is composed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\n",
    "try to maintain this ratio in the sample: 513 female and 487 male. \n",
    "\n",
    "This strategy is called __stratified sampling__ (more [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)). \n",
    "\n",
    "In order to do stratified sampling, the whole dataset is first divided into groups, i.e., _strata_.\n",
    "\n",
    "The sampling procedure then must guarantee that each _stratum_ has a sufficient number of sample.\n",
    "Where sufficient means that there are enough data to characterize the elements belonging to that specific category.\n",
    "\n",
    "\n",
    "In the scenario depicted above, a poor sampling strategy has the 12% chance of providing a strongly unbalanced, thus\n",
    "not meaningful, test set. \n",
    "\n",
    "__Note__: ``unbalanced does not imply not-meaningful\n",
    "\n",
    "\n",
    "Clearly, if we introduce bias at this stage, we compromise all the training process.\n",
    "\n",
    "---\n",
    "\n",
    "What does it mean for a test set to be meaningful in our California housing problem?\n",
    "\n",
    "A meaningful test set should guarantee a sufficient number of samples for every category of households income.\n",
    "(How rich people in a certain neighborhood are?)\n",
    "\n",
    "In order to understand if the test set is well constructed, you may want to compare train set distribution with\n",
    "the one of the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions seem roughly equivalent.\n",
    "Thus we have not introduced any bias so far, at least as far as the median income is concerned.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's focus on a slightly different scenario. \n",
    "\n",
    "Suppose you already know that ``median_income`` is an important feature.\n",
    "\n",
    "In this case you want to be sure that each value for ``median_income`` is\n",
    "sufficiently represented inside the test set, i.e., you want to ensure\n",
    "that each ''category'' of ``median_income``has enough samples in the the test set.\n",
    "\n",
    "The histogram suggests that values are mostly concentrated within 2 and 5.\n",
    "\n",
    "Therefore you want to be sure that this peculiarity persists in the \n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the categories, namely the _strata_,\n",
    "you can perform a stratified sampling with \n",
    " ``StratifiedShuffleSplit``.\n",
    "\n",
    "Each different value for ``income_cat`` is regarded as a stratum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may remove the ``income_cat`` feature in order to get back to the original dataset.\n",
    "\n",
    "__Note__: In this example we assumed that ``median_income`` is an important feature for the problem in hand.\n",
    "For this reason we are allowed to use a stratified sampling strategy wrt to this attribute. \n",
    "\n",
    "However, most of the time, you don't know which may be an important feature, therefore it is difficult to define\n",
    "strata upon which performing a stratified sampling. \n",
    "\n",
    "__That said, you must always be aware that sampling can potentially mess up your entire project!__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and visualize the data to gain insights\n",
    "The following stage is very important. \n",
    "In fact, plotting is probably the best way to visualize and to gain insights about your data.\n",
    "\n",
    "Let's create a copy of the training set in order to keep everything clean. \n",
    "\n",
    "__Note__: We are plotting the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of longitude and latitude\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "save_fig(\"bad_visualization_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty bad right?\n",
    "\n",
    "Remember to be careful about the quality of your plots.\n",
    "\n",
    "Let's change the value of ``alpha``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.scatterplot(data=housing, x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "ax.annotate('Bay Area', xy=(-122.5, 38), xytext=(-124,40 ), xycoords='data',\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "            )\n",
    "ax.annotate('L.A. ', xy=(-118, 34), xytext=(-120,33), xycoords='data',\n",
    "            arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "            )\n",
    "save_fig(\"better_visualization_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to see that most of the houses are concentrated between the Bay Area and around the\n",
    "regions of L.A. and San Diego.\n",
    "\n",
    "Let's try to introduce other information inside the plot.\n",
    "\n",
    "1. The price of the houses -> denoted by the color of the circle, darker is higher\n",
    "2. Population of the district -> denoted by the size of the circle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()\n",
    "save_fig(\"housing_prices_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...fancier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread(PROJECT_ROOT_DIR + '/img/california.png')\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "save_fig(\"california_housing_prices_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_:\n",
    "Try to draw some observation given the plot above.\n",
    "\n",
    "\n",
    "## Looking for Correlations\n",
    "When the dataset is not too large you can compute the *standard correlation coefficient* (also known as the Pearson's coefficient).\n",
    "\n",
    "This coefficient quantify how much two features are correlated with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are interesetd in predicting the ``median_house_value`` we can look at how much the other\n",
    "features are correlated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients range from -1 to 1. A value of -1 means there is a strong negative correlation, while a value of +1 means there is\n",
    "a strong positive correlation.\n",
    "\n",
    "The strength of the correlation gradually fade away as the coefficient approaches to $0$.\n",
    "\n",
    "***\n",
    "__Question__:\n",
    "\n",
    "Pearson's coefficient has a major limitation. What is it?\n",
    "\n",
    "***\n",
    "\n",
    "Now, you might have spotted some important features. \n",
    "Let's try to focus on those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the results.  Which is the most promising feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "save_fig(\"income_vs_house_value_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be concluded by the above plot? ......\n",
    "Also, there are some strange patterns in this figure: the straight line at the top of the plot.\n",
    "What is it?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_low_income = housing[(housing['median_income']<2) & (housing['median_house_value']==housing['median_house_value'].max()) ]\n",
    "housing_low_income.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\", 'ocean_proximity']\n",
    "sns.pairplot(housing[attributes].sample(200), hue='ocean_proximity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations\n",
    "\n",
    "One last thing you may want to do before actually preparing the data for Machine\n",
    "Learning algorithms is to try to combine different attributes together.\n",
    "\n",
    "For instance, the total number of rooms inside a district is not very useful \n",
    "unless we combine this information with the number of households in a district.\n",
    "\n",
    "Indeed, it interesting to look at the number of rooms per household.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other attributes that can be treated as the ``total_rooms`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"rooms_per_household\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check if these transformations bring any improvement to the correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The new attributes are more correlated than the original counterparts.\n",
    "\n",
    "\n",
    "### Recap\n",
    "Keep in mind that gaining insights from the plots, making assumptions based on the data is good. \n",
    "However it is an __iterative__ process. \n",
    "\n",
    "After you have built a prototype of your model, it is always good to go back and explore different paths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms\n",
    "First, you need to separate the Xs from the ys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We know from the previous section there are missing data we need to take care of.\n",
    "\n",
    "``total_bedroom`` has missing values.\n",
    "We can decide wether to fill these values or to drop the entries associated with them.\n",
    "\n",
    "This time, we decide to replace the missing values with the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median()\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of replacing missing values in a dataset is via the class\n",
    "``Imputer`` of ``sklearn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\n",
    "except ImportError:\n",
    "    from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the attributes containing text because the median can only be computed wrt numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "housing_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit the imputer instance to the training data using the ``fit()`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the imputer, after the call to the  ``fit`` method, has only computed\n",
    "the values for for replacing potential NaN in the dataset.\n",
    "\n",
    "The NaN are still in the dataset at this point!\n",
    "\n",
    "Also, the median is compute wrt to every attribute in the dataset. \n",
    "Even though we have missing values only in ``total_bedrooms`` it is preferable to define\n",
    "a replace strategy for every attribute, as it may happen that, at some point, there are missing values in \n",
    "other fields of the dataset.\n",
    "\n",
    "So we are taking precautions!\n",
    "\n",
    "Check that this is the same as manually computing the median of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputer is trained. It can be used to modify the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a plain Numpy array containing the transformed features. \n",
    "If you want a pandas dataframe again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index = list(housing.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[sample_incomplete_rows.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Aside: Scikit-Learn Design**\n",
    "\n",
    "Here are some of the main design princoples of the Scikit-Learn's API\n",
    "\n",
    "\n",
    "* **Consistency**. All objects share a consistent and simple interface\n",
    "    * *Estimators* - Any object that can estimate some parameters based on a dataset\n",
    "       is called an estimator (e.g., an imputer is an estimator). The estimation itself is\n",
    "       performed by the fit() method, and it takes only a dataset as a parameter (or\n",
    "       two for supervised learning algorithms; the second dataset contains the\n",
    "       labels). Any other parameter needed to guide the estimation process is con\n",
    "       sidered a hyperparameter (such as an imputer ’s strategy ), and it must be set\n",
    "       as an instance variable (generally via a constructor parameter).\n",
    "    * *Transformers* - Some estimators (e.g., the imputer) can also transfom a dataset. \n",
    "       The transformation is performed by the ``transform()`` method with the dataset to\n",
    "       transform as a parameter. It returns the transformed dataset.\n",
    "       \n",
    "       All the transformer also have a convenient method called ``fit_transform()`` which performs\n",
    "       both the fit and the transform stages in a single step.\n",
    "    * *Predictors* - some estimators are capable of making predictions given a dataset (e.g., ``LinearRegression``).\n",
    "        A predictor has a ``predict()`` method that takes a\n",
    "        dataset of new instances and returns a dataset of corresponding predictions. It\n",
    "        also has a ``score()`` method that measures the quality of the predictions given\n",
    "        a test set\n",
    "        \n",
    "* **Inspection** - All the estimator’s hyperparameters are accessible directly via public\n",
    "    instance variables (e.g., ``imputer.strategy`` ), and all the estimator’s learned\n",
    "    parameters are also accessible via public instance variables with an underscore\n",
    "    suffix (e.g., ``imputer.statistics_``)     \n",
    "    \n",
    "* **Nonproliferation** of classes. Datasets are represented as NumPy arrays or SciPy\n",
    "sparse matrices, instead of homemade classes. Hyperparameters are just regular\n",
    "Python strings or numbers.\n",
    "\n",
    "* **Composition**. Existing building blocks are reused as much as possible. For\n",
    "example, it is easy to create a Pipeline estimator from an arbitrary sequence of\n",
    "transformers followed by a final estimator, as we will see.\n",
    "\n",
    "* **Sensible defaults**. Scikit-Learn provides reasonable default values for most\n",
    "parameters, making it easy to create a baseline working system quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Text and Categorical Attributes\n",
    "Now let's preprocess the categorical input feature, `ocean_proximity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing.select_dtypes(include=['object'])\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn provides a class for transforming categorical values into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OrdinalEncoder # Scikit-Learn < 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ordinal_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the best way to treat categorical data, though.\n",
    "\n",
    "In fact one issue that may occur is that ML algorithm will assume that two close values have a similar meaning.\n",
    "\n",
    "This is not always the case, especially in our context. \n",
    "\n",
    "In fact the fact  \"<1H OCEAN\" and \"INLAND\" are not similar at all, despite their close corresponding numerical values.\n",
    "\n",
    "For this reason it is always better to transform categorical values according to a one-hot-encoding strategy.\n",
    "\n",
    "Of course, ``sklearn`` provides a transformer: the ``OneHotEncoder``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "except ImportError:\n",
    "    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array  by calling  `toarray()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a custom transformer to add extra attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "\n",
    "Although Scikit-Learn provides many useful transformers, \n",
    "sometimes you may need to write your own transformer.\n",
    "\n",
    "\n",
    "You want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and\n",
    "since Scikit-Learn relies on duck typing (not inheritance), all you need is to create a class and implement three methods: ``fit()``\n",
    "(returning ``self`` ), ``transform()`` , and ``fit_transform()``.\n",
    "\n",
    "You can get the last one for\n",
    "free by simply adding TransformerMixin as a base class. Also, if you add BaseEstimator as a base class \n",
    "(and avoid ``*args`` and ``**kargs`` in your constructor) you will get\n",
    "two extra methods ( ``get_params()`` and ``set_params()``) that will be useful for automatic hyperparameter tuning. \n",
    "For example, here is a small transformer class that adds\n",
    "the combined attributes we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = [\n",
    "    list(housing.columns).index(col)\n",
    "    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            #np.c_ concatenates arrays\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+['roomsPerHouseHolds', 'PopulationPerHouseholds', 'bedRoomsPerRoom']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use Scikit-Learn's `FunctionTransformer` class which lets you easily create a transformer based only on a function. \n",
    "\n",
    "Note that we need to set `validate=False` because the data contains non-float values (`validate` by default is `False` in Scikit-Learn 0.22)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer # it works as a wrapper\n",
    "\n",
    "def add_extra_features(X, add_bedrooms_per_room=True):\n",
    "    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "    population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "    if add_bedrooms_per_room:\n",
    "        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "        return np.c_[X, rooms_per_household, population_per_household,\n",
    "                     bedrooms_per_room]\n",
    "    else:\n",
    "        return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = FunctionTransformer(add_extra_features, validate=False,\n",
    "                                 kw_args={\"add_bedrooms_per_room\": False})\n",
    "housing_extra_attribs = attr_adder.fit_transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_extra_attribs = pd.DataFrame(\n",
    "    housing_extra_attribs,\n",
    "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
    "housing_extra_attribs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "There are two common strategies:\n",
    "1. MinMax Scaling\n",
    "2. Standardization\n",
    "ML algorithms prefer input data with the same scale.\n",
    "Otherwise, the performance of the algorithm can be altered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides two transformers: (i)``MinMaxScaler``, (ii) ``StandardScaler``.\n",
    "\n",
    "Like other transformers they provide the same ``fit()``-> ``transform()`` mechanism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "A  best practice when working with transformers or imputer is to define Pipelines\n",
    "of transformations.\n",
    "\n",
    "Scikit-Learn provides the Pipeline class.\n",
    "\n",
    "Here is a small pipeline for the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline constructor takes a list of <name,estimator> pairs defining a sequence of\n",
    "steps. \n",
    "\n",
    "Every object in a pipeline, except the last one, must be a transformer.\n",
    "\n",
    "The last spot of the pipeline is usually dedicated to either an estimator or a predictor.\n",
    "\n",
    "\n",
    "Calling ``fit()`` on the pipeline implies the call to ``fit_transform()`` on every\n",
    "object in the pipeline, sequentially.\n",
    "\n",
    "The output of a transformer becomes the input to the next object in the pipeline,\n",
    "until the end of the pipeline is reached.\n",
    "\n",
    "So far, we implemented a pipeline for the numerical values.\n",
    "However we have to deal also with not-numerical values.\n",
    "\n",
    "A possible approach is to design two different pipelines, one for each type of values and\n",
    "then using a ``ColumnTransformer``.\n",
    "\n",
    "*How does it work?*\n",
    "This Transformer enables the possibility of applying each transformer (or another Pipeline) \n",
    "to a specific subset of column.\n",
    "\n",
    "Then the result is merged in  a single feature space.\n",
    "\n",
    "A ``ColumnTransformer`` asks for a list of tuples.\n",
    "\n",
    "Each tuple in the list contains: the name of the transformer, the transformer, the column(s) upon which the transformations\n",
    "need to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "except ImportError:\n",
    "    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and train a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! You now have a working Linear Regression model.\n",
    "\n",
    "It is not so hard, is it? \n",
    "\n",
    "Let's use the regressor to make some prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "# the data need to be transformed according to the same pipeline used for the training set\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it against the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works. \n",
    "\n",
    "However, we still do not know how good this model actually is.\n",
    "\n",
    "A classic approach when working with regression problem is to use the RMSE error in order to measure\n",
    "its performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it is not a great score considering that the ``median_housing_values`` ranges between 120k and 265k.\n",
    "\n",
    "Now, compute some other metric over the training set ($l_1$ loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "lin_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, if a model yield poor performance, the first thing to do is to try with another model.\n",
    "\n",
    "\n",
    "(Obviously, you first need to check if your results are correct, i.e., you are not messed up with something in your model or with the data preparation stack)\n",
    "\n",
    "Why don't you try with ``DecisionTreeRegressor``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 0 error! \n",
    "\n",
    "Should you be worried about this result or should you be happy for it?  Are you the legit ML GOAT?\n",
    "\n",
    "## Better Evaluation Using Cross-Validation\n",
    "A better way to evaluate the performance of a model during the training phase\n",
    "is to use cross-validation, as it returns more reliable measures.\n",
    "\n",
    "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
    "function to split the training set into a smaller training set and a validation set, then\n",
    "train your models against the smaller training set and evaluate them against the validation set. \n",
    "It’s a bit of work, but nothing too difficult and it would work fairly well.\n",
    "\n",
    "A great alternative is to use Scikit-Learn’s cross-validation feature. The following code\n",
    "performs K-fold cross-validation: it randomly splits the training set into 10 distinct\n",
    "subsets called folds, then it trains and evaluates the Decision Tree model 10 times,\n",
    "picking a different fold for evaluation every time and training on the other 9 folds.\n",
    "The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"Scores:\", rmse_scores)\n",
    "print(\"Mean: \", rmse_scores.mean())\n",
    "print(\"Std.: \", rmse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn cross-validation features expects a utility function\n",
    "(greater is better) rather than a cost function (lower is better), this is why you need\n",
    "to stick the negative sign.\n",
    "\n",
    "Now the Decision Tree doesn’t look as good as it did earlier.\n",
    "\n",
    "In fact, it seems to be worse than the Linear Regression model! \n",
    "\n",
    "Notice that cross-validation allows you to get not only an estimate of the performance of your model,\n",
    "but it also measures the consistency of your model as the standard deviation of its performance.\n",
    "\n",
    "Now let's do the same thing with the linear regression estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "def print_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Std.: \", scores.std())\n",
    "\n",
    "\n",
    "print_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw some conclusions...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something new. Here your are going to build a ``RandomForestRegressor`` (it is the ``ensamble`` module of sklearn).\n",
    "\n",
    "It is an Ensamble Learning Method. \n",
    "\n",
    "**Note**: you need to specify `n_estimators=10` to avoid a warning about the fact that the default value is going to change to 100 in Scikit-Learn 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute some predictions\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the score of the model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "print_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Is there any sign of overfitting?\n",
    "\n",
    "*Motivate your answer*...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Your Model\n",
    "After you tried a number of different solutions, you end up with a short list of promising models.\n",
    "\n",
    "The goal is now to try to boost their performance via parameter tuning.\n",
    "\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "You  want Scikit-Learn’s ``GridSearchCV`` to search the best configuration for you.\n",
    "\n",
    "For example, the following code aims at finding  the best combination\n",
    "of hyperparameters  for the RandomForestRegressor.\n",
    "\n",
    "It should be noted that each dict  in ``param_grid`` is considered only once. \n",
    "Therefore each dict in the list corresponds to a configuration for the algorithm.\n",
    "\n",
    "Sklearn also considers every possible combination of the selected parameters for the given model, therefore\n",
    "it might required a lot of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the grid search has done, you get the best hyperparameters combination with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a reference directly to the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the scores of each hyperparameter combination tested during the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "Instead of providing each possible combination by hand,\n",
    " you can use ``RandomizedSearchCV``.\n",
    "\n",
    "It is especially useful when you have a lot of hyperparameters, each of which may vary in an large range.\n",
    "\n",
    "It evaluates a given number of random combinations by selecting a random\n",
    "value for each hyperparameter at every iteration. This approach has two main\n",
    "benefits:\n",
    "1. 1.000 iterations will corresponds to 1000 different combination for each hyperparameter\n",
    "2. you have more control over the amount of resources you want to allocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=11),\n",
    "        'max_features': randint(low=1, high=3),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Best Models and Their Errors\n",
    "the RandomForestRegressor is able to indicate the relative importance of each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display these importance scores next to their corresponding attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information in mind, you might want to revisit some of the steps you have included in yours pipeline.\n",
    "\n",
    "For instance, you may want to try dropping some of the less useful features.\n",
    "(e.g., apparently only one ocean_proximity category is really useful, so you could try\n",
    "dropping the others).\n",
    "\n",
    "---\n",
    "### Evaluate Your System on the Test Set \n",
    "Training performance measures how good you model is in terms of __approximation__.\n",
    "\n",
    "However, your ultimate goal is to provide a model that is as good as possible in terms of __generalization__.\n",
    "\n",
    "However, the real test is performed against the test set. \n",
    "\n",
    "Hopefully, the best test on the training data will be the same as the one on the test set.\n",
    "\n",
    "__Careful: Performance reported on the test must no affect the decision process when choosing the final model!__\n",
    "\n",
    "You can take the ``full_pipeline`` we build before, as it is already trained, and call\n",
    "the ``transform`` method in order to transform the data in the test set.\n",
    "\n",
    "**Note:** Be careful! You need to transform the data according to the same parameters of the transformer objects\n",
    "obtained during the training stage. \n",
    "__If you call fit upon the test set, it is an error!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test) #!!!!!!!!! do not call fit or fit_transform!\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to expect from this evaluation?**\n",
    "\n",
    "The performance should roughly match the ones obtained using cross validation.\n",
    "\n",
    "This is not the case, but if that happens, you must avoid the trap of trying to fit the data\n",
    "in the test set, usually the boost in performance will actually be a ''fake\" improvement, since your model\n",
    "will likely fail to generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A full pipeline with both preparation and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"linear\", LinearRegression())\n",
    "    ])\n",
    "\n",
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "full_pipeline_with_predictor.predict(some_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence using joblib\n",
    "One the training phase is done, it is\n",
    "better to save your progresses, i.e., your trained model.\n",
    "\n",
    "Scikit-Learn provides the module ``joblib`` for easily \n",
    "save and restore  python objects in a regular .pickle file.\n",
    "\n",
    "You should always prefer ``joblib`` over other serialization\n",
    "techniques, as it is generally more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = full_pipeline_with_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\") # DIFF\n",
    "#...\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\") # DIFF\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1\n",
    "Let's train a support vector machine regressor mode (`sklearn.svm.SVR`).\n",
    "It has several parameters, e.g., `kernel=\"linear|rbf\"` and others (see the doc file).\n",
    "\n",
    "Find the best possible configuration via parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model achieves the following score (evaluated using 5-fold cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mse = grid_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be worse than the `RandomForestRegressor`. \n",
    "Let's check the best hyperparameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `GridSearchCV` with `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVR(), n_iter=5, n_jobs=4,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x131900e50>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x13199cfd0>,\n",
       "                                        'kernel': ['linear', 'rbf']},\n",
       "                   random_state=42, scoring='neg_mean_squared_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "# see https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n",
    "\n",
    "# Note: gamma is ignored when kernel is \"linear\"\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "svm_reg = SVR()\n",
    "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
    "                                n_iter=5, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, n_jobs=4, random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model achieves the following score (evaluated using 5-fold cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65034.05830317045"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_mse = rnd_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is much closer to the performance of the `RandomForestRegressor` (but not quite there yet). Let's check the best hyperparameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 26290.206464300216, 'gamma': 0.9084469696321253, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we ended up with RBF being the best choice for the kernel function.\n",
    "\n",
    "Randomized search tends to find better hyperparameters than grid search.\n",
    "\n",
    "Let's look at the exponential distribution we used, with `scale=1.0`. Note that some samples are much larger or smaller than 1.0, but when you look at the log of the distribution, you can see that most values are actually concentrated roughly in the range of exp(-2) to exp(+2), which is about 0.1 to 7.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expon_distrib = expon(scale=1.)\n",
    "samples = expon_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Exponential distribution (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Log of this distribution\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution we used for `C` looks quite different: the scale of the samples is picked from a uniform distribution within a given range, which is why the right graph, which represents the log of the samples, looks roughly constant. This distribution is useful when you don't have a clue of what the target scale is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAELCAYAAACRclHqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAApjklEQVR4nO3de7xcVX338c8XkiaaiyQmIhdJHhAMHtpgCcVeQHgABazFh3gJRMpFLuJD1YdSxD4BUi4Fi4/6UtQaBAICCtRARZAWngLeL5Ea2mMCNUoKksAJHJKcBAKUX/9Ya2RnmNu57jNzvu/Xa15nzl577b3W7Jk1v1l77bUVEZiZmZnZyNqu7AKYmZmZjUUOwszMzMxK4CDMzMzMrAQOwszMzMxK4CDMzMzMrAQOwszMzMxK4CBsBEg6UNJDZZejSNLBkh7rx/r3STolP18o6Z+HsCzdkg7OzxdLun4It/3Xkr4yVNur2vZMSaskvWqYtn+ipO8Nx7aHgqRvSDqy7HKYNaLkGkm9kn7SYp6lki5ukN4nafdBlmu2pJA0Lv//bUknDGabhW1v850j6RFJhw3FtvP2fttm2+A4CKuS36zP5g/ZuvxhnDyYbUbEdyPiTUNVxrJFxA0R8fZm6zVryArb64qI+wZbrlqBZUT8bUScMtht13EusDQinh2m7Q85SYdIulfSBkmPtLD+oTnQ3JLzzSokfxJoenxtbBvqAGAA/gQ4HNg1Iv6gOnEgP3YiYnJE/GqoCpi3eWREXNtsvRy4vbHJtobsO6dWOz5UbbY5CKvnXRExGdgXeAvwibIKUvmV1InauW6SJgAnAEPWazdCNgNXA3/VbEVJM4BlwHnAdGA5cFMlPSJ+AkyVNG94imo2JGYBj0TE5rILMhLauV0dixyENRAR64B/IgVjAEh6q6QfSHpG0opil6yk6bnb+/Hc9X1bXr5ND03+ZfgJSb/I610jaWJxXUkfl7QOuEbSBEmfzdt9PD+fUNje0ZJ+LmmjpNWSjsjLT5K0UtImSb+SdHqrdZd0eO4B2SDpCkCFtN/+csxd/Z+R9GTe/79J2kfSacBC4Jzcq3h7oe4fl/QgsFnSuBq/lCdKuimX+wFJcwv73uZXYOVXmqRJwLeBnfP++iTtrKrTm5L+LHelP6N0inXvquNytqQHc71vqhyXGg4AnomI4nE9Mb/OmyT9WtLCQtqphWPxC0m/n5efm49ZZfn/anBM5ki6W9LTkh6S9L5669YTET+JiK8CrfyKPwbojohbIuI5YDEwV9Kcwjr3Ae/sbznMWmjXzpG0NqedUv3Zr9rWzpK+mT8bv5R0al7+QeArwB/mNuFvqvLtDfx9If2ZQvI0SXfkz+aPJe1RyPfbskg6Kn92N0n6jaSz65Rxe0mfkrRe0q+o+txo2yEfb5R0f26H1ku6KS//Tl59RS7v+1X7O6PWcJP9Vfs75xU9gZX6NWnHD2t2HAtl+0ul74i1kk6q9fqMVQ7CGpC0K3Ak8Mv8/y7AHaRTMNOBs4FvSJqZs3wVeDXQBbwO+EyDzS8E3gHsAewFLCqkvT5vfxZwGvB/gbeSgsG5wB9U1pf0B8B1pJ6NHYCDgEfydp4E/hSYCpwEfKby5d+k3pUekEXADGA18Md1Vn973udewGuA9wFPRcQS4Abg73LX/bsKeY4lNUA7RMSLNbZ5NHBLfg1uBG6TNL5RmfOv3COBx/P+JkfE41X12gv4GvAxYCZwJ3C7pN8prPY+4AjgfwC/B5xYZ5e/CxTHXEwCPgccGRFTgD8Cfp7T3ksKYP6cdCz+DHgqZ10NHEh67f4GuF7STtU7y9u/O78erwMWAF+U9Oacfq5SYFnzUf+Va6gLWFH5J7/Gq/PyipWk96RZfzVq144AzgIOA94IHNxkW18HHgN2Bt4D/K2k/xkRVwEfAn6Y24QLipkiYmVV+g6F5AWkz+Q00nfAJXX2fRVwev7c7wP8S531TiW1x28B5uVy1nMR8M9537sCn8/lPSinz83lrfRMV39n1NLoO6emJu14Rd3jWCjba4BdgA8CX5A0rdm+xwoHYbXdJmkT8CgpkKl8cD8A3BkRd0bESxFxN+kUzVH5i/NI4EMR0RsRL0TE/Q32cUVEPBoRT5M+3McW0l4CLoiIrXm80ULgwoh4MiJ6SA3D8XndDwJXR8TduUy/iYhVABFxR0SsjuR+0of6wBbqfxSpB+QfIuIF4LPAujrrvgBMAeYAioiVEbG2yfY/l+tebyzVzwr7/jQwkfQhH6z3A3fk1+oF4FPAq0gBU7Fsj+fjcjuFXtAqOwCbqpa9BOwj6VURsTYiuvPyU0iN2E/zsfhlRKwByL1Mj+djdxPwH6RGrNqfkk6pXBMRL0bEvwLfAN6bt3NZROxQ79GfF6lgMrChatkG0vGu2JRfC7P+atSuvQ+4JiK6I2IL6UdMTZLeQPqR+PGIeC4ifk7q/frzQZbv1txz/CIpENm3znovAG+WNDW3/Q/UWe99wGcL7f6lDfb9Aimg2jnXqdmYtervjFoafecMRqPjCKkuF+bvxDuBPqBjxkgPloOw2t6df9UcTAouZuTls4D3VvUw/AmwE/AG4OmI6G1xH48Wnq8h/YKr6Mmnfyp2zuvUWv8NpN6JV5B0pKQf5S76Z0jB1Yxa61bZuVi+iIiq8lJI+xfgCuALwJOSlkia2mT7NbdVKz0iXuLlX7iDtc3rmLf9KOkXWkUx2NxCCkRq6aUQjOReoveTflWvzacxKqftGh2jP1c6lVx5P+1D7WM0Czig6r23kPQrc7j0kXruiqaybfA5BXhmGMtgnatRu7ZNG0TjNmNnUttbfF+uYdvP9UC02hbMJ7Wta/IpxD9sUM7qdr+ec0hDQH6iNHzi5CZlrf7OqKXRd85gNDqOkM6MFM94NHotxxwHYQ3k3qOlpB4TSG/ir1b1MkyKiMty2nRJO7S4+TcUnu8GFE+dRdW6j5O+hGut/yipe3kb+Zz8N3LZd8y9IXdSGNvVwNpi+SSpqrzbiIjPRcR+wJtJ3dyVQd/V9aDJ8orivrcjdcdX6ruFdMq3ohiENNvuNq9joV6/aZKvlgdJdX155xH/FBGHk4LyVcCVOaneMZqV1zkTeG0+Rv9O7WP0KHB/1XtvckSckbf113p5LNwrHgOoH0A3hVON+ZToHnl5xd4UTlma9UOjdm0t6XNfUbf9yXmmSyr20O5G65/rZu1G48yph/to0jCB24Cb66y6TbtKKmO9ba6LiFMjYmfgdNLQg0ZXRLZSh3rfOZsptKmSqn/Y9atd5ZXfZ9aAg7DmPgscrjQ4/HrgXZLekQdZTswDD3fNp+C+TfqwTJM0XtJBDbb7vyXtKmk66Zz6TQ3W/RqwSGleqhnA+bx8Vd5VwElKUwlsJ2mX3APzO8AEoAd4UWk+p6bTSmR3AF2SjlG60uYj1OlxkbS/pAPymK3NwHOkrnGAJ4CBzKWzX2HfHwO2Aj/KaT8Hjsuv/xHA2wr5ngBeK+k1dbZ7M/DO/FqNB/4yb/sHAyjjT4Ad8jhBJO2odIHEpLzNPl5+Hb4CnC1pPyVvzAHYJFID15O3cRKpJ6yWbwF7STo+v7fG59d+b/jtVByT6z0qG8nvkYnA+PSvJlaNiSu6lXR6dX7Ocz7wYOV0d/Y20vverJHx+b1WeYyjcbt2M6ld21vSq0lX6NYUEY+SPsOX5m3/HmmYRqtXLj8B7Nrgc1CXpN9RmjfxNXmIw0Ze/txXuxn4SG73p5GmuKm33fcqjUmG1OseDL5drfeds4LU3u+bP+eLq/I121+j42hNOAhrIp/jvg44P3/Yjwb+mvTF+Sip16fyOh5POv+9ijSW7GMNNn0jaYzWr0inqhrNt3QxaezZg8C/AQ9U1o80TcBJpIsANgD3A7Ny1/xHSB/8XuA44Jst1nk9aazRZaQB5HsC36+z+lRSb04vqRv6KeDynHYVaazEM8pXirboH0mn9npJr+kxuYED+CjwLtIpsIWkX56Vcq8iNQi/yvvcprs9Ih4ijev7PLA+b+ddEfF8P8pW2dbzpF7SD+RF25EGEj8OPE0KTs7I695CGoNxI+lU3m3A9Ij4BfD/gB+SGrrfpc7rnI/n20mDhR8nnSr5JCnQ7o+DgGdJvaK75ee/nXg3n/pYmPfZQzrVcgnpWByQ919Zd3+gL78HzRq5k/ReqzwW07hd+zbpQpd7SYPiKz/CttbZ/rHAbNJn41bS+Kh7Wizbv5B6d9dJWt+POlUcDzwiaSNpOMLCOutdSbrafgWprssabHN/4Me5F/ubwEfj5XnJFgPX5jauP1dI1/zOiYiHgQuBe0hjUqvHnzVrx+seR2tOabiPjSSlSTJP6UcjYaOQ0lWx3wXe0mAwbMeS9A3gqjzY1mzY5B7ffwcmRO0rqs3akid1Mxug3FM0p+mKHSoi5pddButcSnPm3Ukar/RJ4HYHYNZpfDrSzMxGo9NJwzpWA/9FPr1v1kl8OtLMzMysBO4JMzMzMytB240JmzFjRsyePbvsYpjZCPrZz362PiJmNl9z9HMbZja2NGq/2i4Imz17NsuXLy+7GGY2giQ1ml28rbgNMxtbGrVfLZ+OlLRA0kpJmyWtlnRgXn6opFWStki6N09CWckzQdLVkjZKWifprKpt1s1rZmZm1slaCsIkHU66RPgk0r3iDiJNiDmDNOHceaQ7uC9n25nfF5Mm+pwFHAKck2c5p4W8ZmZmZh2r1Z6wvyHdBf1HEfFSRPwmIn4DHAN0R8Qt+eahi4G5evnGxScAF+U7y68kzRh8Yk5rltfMzMysYzUNwiRtD8wDZkr6paTHJF0h6VVAF4Wb90bEZtKcLl353lg7se3NfVfkPDTKW6MMp0laLml5T09Pf+toZmZmNuq00hO2I+lmv+8BDgT2Bd4CLAImk+5XWLSBdMpycuH/6jSa5N1GRCyJiHkRMW/mzI64QMrMzMzGuFaCsMo98T4fEWvzzZ0/DRwF9JFu4Fw0lXST4r7C/9VpNMlrZmZm1tGaBmER0Qs8BhSn1q887wbmVhZKmgTsQRrr1QusLabn593N8va7FmZmZmZtptWB+dcAfyHpdXms1/8BvgXcCuwjab6kicD5wIMRsSrnuw5YJGlaHnB/KrA0pzXLa2ZmZtaxWg3CLgJ+CjwMrAT+FbgkInqA+cAlQC9wALCgkO8C0mD7NcD9wOURcRdAC3nNzMzMOlZLM+ZHxAvAh/OjOu0eoOa0EhGxFTg5P2ql1807VGafe8c2/z9y2TuHc3dmZmY1+fvIqvkG3mZmZmYlcBBmZmZmVgIHYWZmZmYlcBBmZmZmVgIHYWZmZmYlaOnqSDMzs05XffUi+ApGG17uCTMzMzMrgXvCzMzMrBRjfe4094SZmZmZlcA9YWZmHcZjm8zag4MwMzMrxVg/FWXmIMzMzGwMco9p+TwmzMzMzKwEDsLMzMzMSuAgzMzMzKwEDsLMzMzMSuAgzMzMzKwEvjrSzMwATxlhNtIchJmZWcs8rUFrar1OZtV8OtLMzMysBO4JMzMz6weftrWh4iDMzMw6TrsGSu1a7nZV9ul1B2FmZm3MY49sLOqUYNVBmJmZjUntGsCW3XtjQ8cD883MzMxK0FIQJuk+Sc9J6suPhwppx0laI2mzpNskTS+kTZd0a05bI+m4qu3WzWtmZmbWyfpzOvLMiPhKcYGkLuDLwDuBB4AlwBeBBXmVLwDPAzsC+wJ3SFoREd0t5DUzG1aSJpDancOA6cBq4BMR8e2cfiipHdsN+DFwYkSsKeT9EvAeYAvwdxHx6RGvxChQ9vicdj2taDbY05ELgdsj4jsR0QecBxwjaYqkScB84LyI6IuI7wHfBI5vlneQZTIza9U44FHgbcBrgEXAzZJmS5oBLCO1TdOB5cBNhbyLgT2BWcAhwDmSjhi5optZu+tPEHappPWSvi/p4LysC1hRWSEiVpN6vvbKjxcj4uHCNlbkPM3ympkNu4jYHBGLI+KRiHgpIr4F/BrYDzgG6I6IWyLiOVLQNVfSnJz9BOCiiOiNiJXAlcCJI18LM2tXrZ6O/DjwC1KQtAC4XdK+wGRgQ9W6G4ApwH8BG+uk0STvNiSdBpwGsNtuu7VYZDOz/pG0I+mHYDdwBtv+UNwsaTXQJekJYKdien7+7uEu40BPvZV9ytAGZihPtfq07ejTUhAWET8u/HutpGOBo4A+YGrV6lOBTcBLDdJokrd6/0tIY8aYN29etFJmM7P+kDQeuAG4NiJWSZoM9FStVvmhOLnwf3VarW2Puh+S/kI2q20kf7AMdJ6wAET6tTi3slDS7sAE4GFSEDZO0p4R8R95lbk5D03ympmNGEnbAV8l9fafmRc3+qHYV/j/uaq0V/APydZ4/isbTqPxh0fTIEzSDsABwP3Ai8D7gYOAjwLjgR9KOpB0heOFwLKI2JTzLgMulHQK6erIo4E/ypu+oVFeM7ORIEnAVaSruI+KiBdyUjdp3FdlvUnAHqRxYr2S1pJ+SN6dVyn+yBwSo/FLY6T5NOroM5zB8lgLxFvpCRsPXAzMIY3zWgW8uzLgXtKHSAHVa4F7gJMKeT8MXA08CTwFnBER3QB5mopGec3MRsKXgL2BwyLi2cLyW4HLJc0H7gDOBx6MiFU5/TpgkaTlpADuVNyGAWPvi9RsoJoGYRHRA+zfIP1G4MY6aU/TYKBqo7xmZsNN0izgdGArsC51igFwekTckAOwK4DrSfOEFecxvIAUwK0BngU+GRF3jVTZ24179YbXUPUYOoAeWb53pJmNWXniVTVIv4d0FqBW2lbg5PywEeJgzjqJgzAzM7M62jXoa9dyjzW+gbeZmZlZCdwTZmZmHc89QzYaOQgzMzOzuhzADh+fjjQzMzMrgYMwMzMzsxL4dKSZmZmNWp18OtQ9YWZmZmYlcE+YmZmZDamRnnm/XWf6d0+YmZmZWQncE2ZmZjV18lgcs1YNZy+be8LMzMzMSuCeMDMzsxKMtZ7GsVbfVrgnzMzMzKwEDsLMzMzMSuAgzMzMzKwEHhNmZmZmHacdxqA5CDMzMxuEdviyH25+DQbGpyPNzMzMSuAgzMzMzKwEDsLMzMzMSuAgzMzMzKwEDsLMzMzMSuAgzMzMzKwEDsLMzMzMStCvIEzSnpKek3R9YdlxktZI2izpNknTC2nTJd2a09ZIOq5qe3XzmpmZmXWy/vaEfQH4aeUfSV3Al4HjgR2BLcAXq9Z/PqctBL6U87SS18zMzKxjtTxjvqQFwDPAD4A35sULgdsj4jt5nfOAlZKmAC8B84F9IqIP+J6kb5KCrnMb5Y2ITUNROTMzM7PRqqWeMElTgQuBs6qSuoAVlX8iYjWp52uv/HgxIh4urL8i52mWt3r/p0laLml5T09PK0U2MzMzG9VaPR15EXBVRDxWtXwysKFq2QZgSk7bWCetWd5tRMSSiJgXEfNmzpzZYpHNzMzMRq+mpyMl7QscBrylRnIfMLVq2VRgE+l0ZL20ZnnNzMzMOlorY8IOBmYD/ykJUg/W9pLeDNwFzK2sKGl3YALwMCkIGydpz4j4j7zKXKA7P+9ukNfMzMyso7UShC0Bvl74/2xSUHYG8Drgh5IOBB4gjRtbVhlYL2kZcKGkU4B9gaOBP8rbuaFRXjMzM7NO1nRMWERsiYh1lQfpNOJzEdETEd3Ah0gB1ZOk8VwfLmT/MPCqnPY14IychxbympmZmXWsfs+YHxGLI+IDhf9vjIjdImJSRBwdEU8X0p6OiHfntN0i4saqbdXNa2Y23CSdma+83ippaWH5bEkhqa/wOK+QPkHS1ZI2SlonqfrKcTOzplqeJ8zMrAM9DlwMvIPUa19th4h4scbyxcCewCzg9cC9kn4REXcNV0HNrPP43pFmNmZFxLKIuA14qp9ZTwAuiojeiFgJXAmcOMTFM7MO5yDMzKy+NZIek3SNpBkAkqYBO1GYbJptJ6I2M2uJgzAzs1daD+xPOt24H+nCoRty2uT8tzjZdM2Jpit81w8zq8VBmJlZlYjoi4jlEfFiRDwBnAm8Pd8Xty+vVpxsuuFE077rh5nV4iDMzKy5yH+3i4heYC2FyabZdiJqM7OWOAgzszFL0jhJE4HtSXcCmZiXHSDpTZK2k/Ra4HPAfRFROQV5HbBI0jRJc4BTgaWlVMLM2paDMDMbyxYBzwLnAh/IzxcBu5Nuy7YJ+HdgK3BsId8FwGpgDXA/cLmnpzCz/vI8YWY2ZkXEYtKcX7V8rUG+rcDJ+WFmNiDuCTMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxI4CDMzMzMrgYMwMzMzsxK0FIRJul7SWkkbJT0s6ZRC2qGSVknaIuleSbMKaRMkXZ3zrZN0VtV26+Y1MzMz62St9oRdCsyOiKnAnwEXS9pP0gxgGXAeMB1YDtxUyLcY2BOYBRwCnCPpCIAW8pqZmZl1rHGtrBQR3cV/82MPYD+gOyJuAZC0GFgvaU5ErAJOAE6MiF6gV9KVwInAXcAxTfKamZmZdayWx4RJ+qKkLcAqYC1wJ9AFrKisExGbgdVAl6RpwE7F9Py8Kz+vm3dANTEzMzNrIy0HYRHxYWAKcCDpNOJWYDKwoWrVDXm9yYX/q9Nokncbkk6TtFzS8p6enlaLbGZmZjZq9evqyIj4r4j4HrArcAbQB0ytWm0qsCmnUZVeSaNJ3ur9LomIeRExb+bMmf0pspmZmdmoNNApKsaRxoR1A3MrCyVNqizP48DWFtPz88r4srp5B1gmMzMzs7bRNAiT9DpJCyRNlrS9pHcAxwL/H7gV2EfSfEkTgfOBBwsD668DFkmaJmkOcCqwNKc1y2tmZmbWsVrpCQvSqcfHgF7gU8DHIuKbEdEDzAcuyWkHAAsKeS8gDbZfA9wPXB4RdwG0kNfMzMysYzWdoiIHS29rkH4PMKdO2lbg5PzoV14zMzOzTubbFpmZmZmVwEGYmZmZWQkchJmZmZmVwEGYmZmZWQkchJmZmZmVwEGYmZmZWQkchJmZmZmVwEGYmZmZWQkchJmZmZmVwEGYmY1Zks6UtFzSVklLq9IOlbRK0hZJ90qaVUibIOlqSRslrZN01ogX3szanoMwMxvLHgcuBq4uLpQ0A1gGnAdMB5YDNxVWWQzsCcwCDgHOkXTECJTXzDqIgzAzG7MiYllE3AY8VZV0DNAdEbdExHOkoGuupMq9bk8ALoqI3ohYCVwJnDgypTazTuEgzMzslbqAFZV/ImIzsBrokjQN2KmYnp931duYpNPyac/lPT09w1RkM2s3DsLMzF5pMrChatkGYEpOoyq9klZTRCyJiHkRMW/mzJlDWlAza18OwszMXqkPmFq1bCqwKadRlV5JMzNrmYMwM7NX6gbmVv6RNAnYgzROrBdYW0zPz7tHtIRm1vYchJnZmCVpnKSJwPbA9pImShoH3ArsI2l+Tj8feDAiVuWs1wGLJE3Lg/VPBZaWUAUza2MOwsxsLFsEPAucC3wgP18UET3AfOASoBc4AFhQyHcBaaD+GuB+4PKIuGsEy21mHWBc2QUwMytLRCwmTT9RK+0eYE6dtK3AyflhZjYg7gkzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK0HTIEzSBElXSVojaZOkn0s6spB+qKRVkrZIulfSrKq8V0vaKGmdpLOqtl03r5mZmVkna6UnbBzwKPA24DWkGaZvljRb0gxgGXAeMB1YDtxUyLsY2BOYBRwCnCPpCIAW8pqZmZl1rKYz5kfEZradUfpbkn4N7Ae8lnRD21sAJC0G1kuak++xdgJwYr7hba+kK4ETgbuAY5rkNTMzM+tY/R4TJmlHYC+gG+gCVlTScsC2GuiSNA3YqZien3fl53Xz1tjnaZKWS1re09PT3yKbmZmZjTr9CsIkjQduAK7NvVWTgQ1Vq20ApuQ0qtIraTTJu42IWBIR8yJi3syZM/tTZDMzM7NRqeUgTNJ2wFeB54Ez8+I+YGrVqlOBTTmNqvRKWrO8ZmZmZh2tpSBMkoCrgB2B+RHxQk7qBuYW1psE7EEa69ULrC2m5+fdzfIOqCZmZmZmbaTVnrAvAXsD74qIZwvLbwX2kTRf0kTgfODBwsD664BFkqZJmgOcCixtMa+ZmZlZx2plnrBZwOnAvsA6SX35sTAieoD5wCVAL3AAsKCQ/QLSYPs1wP3A5RFxF0ALec3MzMw6VitTVKwB1CD9HmBOnbStwMn50a+8ZmZmZp3Mty0yMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDMzM7MSOAgzMzMzK4GDMDOzBiTdJ+k5SX358VAh7ThJayRtlnSbpOllltXM2ouDMDOz5s6MiMn58SYASV3Al4HjgR2BLcAXSyyjmbWZcWUXwMysTS0Ebo+I7wBIOg9YKWlKRGwqt2hm1g7cE2Zm1tylktZL+r6kg/OyLmBFZYWIWA08D+xVnVnSaZKWS1re09MzEuU1szbgIMzMrLGPA7sDuwBLgNsl7QFMBjZUrbsBmFK9gYhYEhHzImLezJkzh7u8ZtYmHISZmTUQET+OiE0RsTUirgW+DxwF9AFTq1afCvhUpJm1xEGYmVn/BCCgG5hbWShpd2AC8HBJ5TKzNuOB+WZmdUjaATgAuB94EXg/cBDwUWA88ENJBwIPABcCyzwo38xa1VJPmKQz86DSrZKWVqUdKmmVpC2S7pU0q5A2QdLVkjZKWifprFbzmpmNAuOBi4EeYD3wF8C7I+LhiOgGPgTcADxJGgv24bIKambtp9XTkY+TGqKriwslzQCWAecB04HlwE2FVRYDewKzgEOAcyQd0WJeM7NSRURPROwfEVMiYoeIeGtE3F1IvzEidouISRFxdEQ8XWZ5zay9tBSERcSyiLgNeKoq6RigOyJuiYjnSEHXXElzcvoJwEUR0RsRK4ErgRNbzGtmZmbWsQY7ML96npzNwGqgS9I0YKdien7e1Sxv9U48x46ZmZl1msEGYY3myZlc+L86rVnebXiOHTMzM+s0gw3CGs2T01f4vzqtWV4zMzOzjjbYIKx6npxJwB6ksV69wNpien7e3SzvIMtkZmZmNuq1OkXFOEkTge2B7SVNlDQOuBXYR9L8nH4+8GBErMpZrwMWSZqWB9yfCizNac3ympmZmXWsVnvCFgHPAucCH8jPF0VEDzAfuAToJU1quKCQ7wLSYPs1pMkOL4+IuyBd+t0kr5mZmVnHamnG/IhYTJpColbaPUDNaSUiYitwcn70K6+ZmZlZJ/O9I83MzMxK4CDMzMzMrARj7gbes8+94xXLHrnsnSWUxMzMzMYy94SZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJxtxti2rxrYzMzMxspLknzMzMzKwEDsLMzMzMSuAgzMzMzKwEDsLMzMzMSuCB+XVUD9b3QH0zMzMbSu4JMzMzMyuBgzAzMzOzEvh0ZIs8l5iZmZkNJQdhg+DAzMzMzAbKQdgQa2VAvwf9m5mZmYOwYVart8zMzMzMQdgo0Gqg5h4zMzOzzlF6ECZpOnAV8HZgPfCJiLix3FKNTiPZq+aAz6w5t19mNhilB2HAF4DngR2BfYE7JK2IiO5SSzXGDTTga2UMXKv5WtEOgakv4Ohobr/MbMBKDcIkTQLmA/tERB/wPUnfBI4Hzi2zbDYwAw2K2mHs3FCWsR3qO5w6IQh1+2Vmg1V2T9hewIsR8XBh2QrgbcWVJJ0GnJb/7ZP0UD/2MYN0mqDTjYV6joU6whiopz4J9K+es4atMAPXUvsFg27DhlunvN86pR7QOXXp2HrkNqxVdduvsoOwycDGqmUbgCnFBRGxBFgykB1IWh4R8wZWvPYxFuo5FuoIrmcbaan9gsG1YcOtA44D0Dn1gM6pi+vRXNm3LeoDplYtmwpsKqEsZmb94fbLzAal7CDsYWCcpD0Ly+YCHtRqZqOd2y8zG5RSg7CI2AwsAy6UNEnSHwNHA18dwt2MylMAw2As1HMs1BFcz7YwQu3XSGjr41DQKfWAzqmL69GEImK4tt1aAdI8O1cDhwNPAed6nh0zawduv8xsMEoPwszMzMzGorLHhJmZmZmNSQ7CzMzMzErQsUGYpOmSbpW0WdIaSceVXaZ6JN0n6TlJffnxUCHtuFz+zZJuy2NQKmkN6ziYvENQpzMlLZe0VdLSqrRDJa2StEXSvZJmFdImSLpa0kZJ6ySdNRJ5h7qekmZLisIx7ZN0XjvWM+/vqvw+2STp55KOHO6ylnE8xyJJe+b25/qyyzIYkhZIWpnbtNWSDiy7TP2V2407JfXm9/wVksqez7Ohgbb1o02Dtvytku6W9LSkHkm3SNppyHYcER35AL4G3ESaUPFPSJModpVdrjplvQ84pcbyLtKcQwfletwIfL2VOg4m7xDV6Rjg3cCXgKWF5TPyvt4LTAQuB35USL8U+C4wDdgbWAccMdx5h6Ges4EAxtXJ1zb1BCYBi3OdtgP+NL+3Znfa8RyLD+Cf8+t8fdllGUQdDgfWAG/N79FdgF3KLtcA6nEnsDS/p18P/BvwkbLL1aTMA2rrR9ujQT2OzHWYCryadCHOXUO237IrPkwv5iTSTXX3Kiz7KnBZ2WWrU977qB2E/S1wY+H/PXK9pjSr42DyDnHdLq56Q58G/KDqWD0LzMn/Pw68vZB+ETl4HM68w1DP2TQOwtqynoXtPki6b2JHHs+x8gAWADeTgux2DsJ+AHyw7HIMQT1WAkcV/r8c+HLZ5Wqx7P1q60fro7oeNdJ/H9g0VPvr1NOR9e7p1lVSeVpxqaT1kr4v6eC8rItUbgAiYjU5eKJ5HQeTdzhVl2szsBrokjQN2KmYTuM6DUneIalVfWskPSbpGkkzANq9npJ2JL2HuoerrKOhnp1O0lTgQuCsZuuOZpK2B+YBMyX9Mn/erpD0qrLLNgCfBRZIerWkXUi9MHeVW6QB69TP6EEM4YTMnRqEtXxPt1Hi48DupC70JcDtkvYg1WND1bqVejSr42DyDqdm5aIqvT91Gmje4bAe2J9049b98n5uKJSlsv9aZRm19ZQ0nlSPayNi1TCWdbQdz050EXBVRDxWdkEGaUdgPPAe4EBgX+AtwKISyzRQ3yEFKRuBx4DlwG1lFmgQOu4zKun3gPOBvxqqbXZqENZW93SLiB9HxKaI2BoR1wLfB46icT2a1XEweYdTs3JRld6fOg0075CLiL6IWB4RL0bEE8CZwNslTRlkWUurp6TtSKetnyfVZzjLOqqOZ6eRtC9wGPCZkosyFJ7Nfz8fEWsjYj3waVIb2jby5+su0l0YJpHGVE0DPllmuQahoz6jkt4IfBv4aER8d6i226lBWLvf0y0Akco7t7JQ0u7ABFL9mtVxMHmHU3W5JpHGq3VHRC+wtphO4zoNSd4hqVVzlVmRt2vHekoScBWp12F+RLwwnGVtg+PZ7g4mjVv8T0nrgLOB+ZIeKLNQA5HfK4/x8meMquftYjqwG3BF/kH+FHANbRZMFnTMZzRf1XkPcFFEDO1tycoeBDeMg+u+TroCcBLwx4zSqyOBHYB3kK4eGQcsBDaTxtxUuqUPzPW4nm2vcKxbx8HkHaJ6jct1upTUe1Kp38y8r/l52SfZ9oq4y4D7Sb8A55C+iCtXxA1b3mGo5wHAm0g/dF5LuhL13jau598DPwImVy3vqOM5Vh6kq7xeX3h8CvgHYGbZZRtgfS4Efgq8Lr9fvkv6wiy9bP2sx6+Ac3MbsgNwK4ULrEbjo0Eb2Faf0Qb12IU0lu3sYdlv2RUfxhd0Oulc+mbgP4Hjyi5TnXLOzI3HJuCZ/EV3eCH9uFz+zcA/AtNbreNg8g5BvRaTfo0WH4tz2mHAKtJphPuA2YV8E0iXAG8EngDOqtrusOQd6noCxwK/zq/vWuA64PXtWE/SuLYAniOdYqg8Fnba8RyrD9r/6sjxwBdJbeg64HPAxLLLNYB67Jvfy72kcaU3AzuWXa4mZa7ZBua0tvmMNmjLL8jPi21f31Dt1/eONDMzMytBp44JMzMzMxvVHISZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJHISZmZmZlcBBmJmZmVkJHISZmZmZleC/AfpfHS4+H4+fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reciprocal_distrib = reciprocal(20, 200000)\n",
    "samples = reciprocal_distrib.rvs(10000, random_state=42)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Reciprocal distribution (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Log of this distribution\")\n",
    "plt.hist(np.log(samples), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (indeed, as you can see on the figure on the right, all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new transformer into the preparation pipeline. \n",
    "It selects the most important attributes.\n",
    "\n",
    "The feature importance scores can be computed by any estimator object (e.g., the ``Random Forest Regressor``).\n",
    "\n",
    "It is always better to provide the scores to the transformer, as opposed to let it compute them at every stage of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the number of top features we want to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look for the indices of the top k features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  9, 10, 12])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_feature_indices = indices_of_top_k(feature_importances, k)\n",
    "top_k_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['longitude', 'median_income', 'pop_per_hhold', 'bedrooms_per_room',\n",
       "       'INLAND'], dtype='<U18')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(attributes)[top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that these are indeed the top k features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.31498981654442865, 'median_income'),\n",
       " (0.16102983971113785, 'INLAND'),\n",
       " (0.10927744833331542, 'pop_per_hhold'),\n",
       " (0.09850366376132488, 'bedrooms_per_room'),\n",
       " (0.07241936267111387, 'longitude')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, attributes), reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "Now you can create a new pipeline which includes the ``TopFeatureSelector``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the features of the first 3 instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94135046, -0.8936472 ,  0.00622264, -0.12236839,  1.        ],\n",
       "       [ 1.17178212,  1.292168  , -0.04081077, -0.76455102,  0.        ],\n",
       "       [ 0.26758118, -0.52543365, -0.07537122, -0.32454514,  1.        ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared_top_k_features[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's double check that these are indeed the top k features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94135046, -0.8936472 ,  0.00622264, -0.12236839,  1.        ],\n",
       "       [ 1.17178212,  1.292168  , -0.04081077, -0.76455102,  0.        ],\n",
       "       [ 0.26758118, -0.52543365, -0.07537122, -0.32454514,  1.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared[0:3, top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything's correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single pipeline responsible of the following steps:\n",
    "1. preparation\n",
    "2. feature selection\n",
    "3. training of a model of your choice (``SVR``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('svm_reg', SVR(**rnd_search.best_params_))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preparation',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('attribs_adder',\n",
       "                                                                   FunctionTransformer(func=<function add_extra_features at 0x130de7310>)),\n",
       "                                                                  ('std_scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['longitude', 'latitude',\n",
       "                                                   'housing_median_age',\n",
       "                                                   'total_rooms',\n",
       "                                                   'total_bedrooms',\n",
       "                                                   'population', 'households',\n",
       "                                                   'median_income...\n",
       "                 TopFeatureSelector(feature_importances=array([7.24193627e-02, 6.65224337e-02, 4.38674426e-02, 1.76711852e-02,\n",
       "       1.60591173e-02, 1.70288856e-02, 1.61630622e-02, 3.14989817e-01,\n",
       "       5.06200233e-02, 1.09277448e-01, 9.85036638e-02, 9.94786999e-03,\n",
       "       1.61029840e-01, 3.79708166e-05, 2.57530891e-03, 3.28656932e-03]),\n",
       "                                    k=5)),\n",
       "                ('svm_reg',\n",
       "                 SVR(C=26290.206464300216, gamma=0.9084469696321253))])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the full pipeline on a few instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\t [ 78482.05832018 292717.95095184  86791.45725513 147614.05637917]\n",
      "Labels:\t\t [72100.0, 279600.0, 82700.0, 112500.0]\n",
      "[CV] END C=629.782329591372, gamma=3.010121430917521, kernel=linear; total time=   8.7s\n",
      "[CV] END C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf; total time=  18.0s\n",
      "[CV] END C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf; total time=  17.8s\n",
      "[CV] END C=432.37884813148855, gamma=0.15416196746656105, kernel=linear; total time=   8.7s\n",
      "[CV] END C=432.37884813148855, gamma=0.15416196746656105, kernel=linear; total time=   8.0s\n",
      "[CV] END C=24.17508294611391, gamma=3.503557475158312, kernel=rbf; total time=  15.7s\n",
      "[CV] END C=629.782329591372, gamma=3.010121430917521, kernel=linear; total time=   8.7s\n",
      "[CV] END C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf; total time=  18.0s\n",
      "[CV] END C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf; total time=  15.4s\n",
      "[CV] END C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf; total time=  15.8s\n",
      "[CV] END C=432.37884813148855, gamma=0.15416196746656105, kernel=linear; total time=   8.6s\n",
      "[CV] END C=24.17508294611391, gamma=3.503557475158312, kernel=rbf; total time=  15.1s\n",
      "[CV] END C=629.782329591372, gamma=3.010121430917521, kernel=linear; total time=   8.8s\n",
      "[CV] END C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf; total time=  18.4s\n",
      "[CV] END C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf; total time=  15.5s\n",
      "[CV] END C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf; total time=  16.0s\n",
      "[CV] END C=432.37884813148855, gamma=0.15416196746656105, kernel=linear; total time=   8.5s\n",
      "[CV] END C=24.17508294611391, gamma=3.503557475158312, kernel=rbf; total time=  15.3s\n",
      "[CV] END C=629.782329591372, gamma=3.010121430917521, kernel=linear; total time=   8.7s\n",
      "[CV] END C=629.782329591372, gamma=3.010121430917521, kernel=linear; total time=   8.1s\n",
      "[CV] END C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf; total time=  18.3s\n",
      "[CV] END C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf; total time=  15.5s\n",
      "[CV] END C=432.37884813148855, gamma=0.15416196746656105, kernel=linear; total time=   8.3s\n",
      "[CV] END C=24.17508294611391, gamma=3.503557475158312, kernel=rbf; total time=  15.9s\n",
      "[CV] END C=24.17508294611391, gamma=3.503557475158312, kernel=rbf; total time=  14.5s\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full pipeline works fine. \n",
    "\n",
    "You can try a separate pipeline with a more powerful model like the ``RandomForestRegressor``.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "name": "scikit_intro.ipynb",
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": null,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
